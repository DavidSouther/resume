1:HL["/_next/static/css/fef2051bebc1224a.css",{"as":"style"}]
0:["jrxx5TE6P5ajBZMw_BUzl",[[["",{"children":["blog",{"children":[["id","knowledge_that_by_of","d"],{"children":["__PAGE__?{\"id\":\"knowledge_that_by_of\"}",{}]}]}]},"$undefined","$undefined",true],"$L2",[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/fef2051bebc1224a.css","precedence":"next"}]],"$L3"]]]]
4:HL["/_next/static/css/188c8aff756c3f67.css",{"as":"style"}]
5:I{"id":7767,"chunks":["272:static/chunks/webpack-aeac45c82ebe15f5.js","971:static/chunks/fd9d1056-edf0048f965d11ef.js","596:static/chunks/596-e3269281f6a80cbc.js"],"name":"default","async":false}
6:I{"id":7920,"chunks":["272:static/chunks/webpack-aeac45c82ebe15f5.js","971:static/chunks/fd9d1056-edf0048f965d11ef.js","596:static/chunks/596-e3269281f6a80cbc.js"],"name":"default","async":false}
9:I{"id":6852,"chunks":["13:static/chunks/13-a14f4f8488530559.js","599:static/chunks/599-57c8542fa14d7dc7.js","185:static/chunks/app/layout-a5c39b24d3810515.js"],"name":"","async":false}
2:[null,["$","html",null,{"lang":"en","children":[["$","head",null,{"children":["$","link",null,{"rel":"stylesheet","href":"/jiffies-css-bundle.min.css"}]}],["$","body",null,{"className":"container","children":[["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"childProp":{"current":["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children",["id","knowledge_that_by_of","d"],"children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$L7","$L8",null],"segment":"__PAGE__?{\"id\":\"knowledge_that_by_of\"}"},"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/188c8aff756c3f67.css","precedence":"next"}]]}],"segment":["id","knowledge_that_by_of","d"]},"styles":[]}],"segment":"blog"},"styles":[]}],["$","$L9",null,{}]]}]]}],null]
3:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Beyond Knowledge-That: LLMs' Indirect Understanding - David Souther"}],["$","meta","2",{"name":"description","content":"davidsouther.com - resume, blog, playground"}],["$","link","3",{"rel":"author","href":"davidsouther.com"}],["$","meta","4",{"name":"author","content":"David Souther"}],["$","link","5",{"rel":"manifest","href":"/manifest.json"}],["$","meta","6",{"name":"theme-color","media":"(prefers-color-scheme: light)","content":"white"}],["$","meta","7",{"name":"theme-color","media":"(prefers-color-scheme: dark)","content":"black"}],["$","meta","8",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","9",{"name":"twitter:card","content":"summary"}]]
a:I{"id":7839,"chunks":["548:static/chunks/app/blog/[id]/page-cb8d01a304d3bccf.js"],"name":"Card","async":false}
b:Ta8e,<p>Why I Can Say "The AI Knows" but Still Claim "It Doesn't Really Know"?</p>
<p>We often talk about AI language models like they have real knowledge: "ChatGPT said the capital of France is Paris" or "Claude knows a lot about quantum physics." But then we also say things like "Language models don't actually understand what they're saying" or "They have no real knowledge, just patterns in data."</p>
<p>This seems like a contradiction, but there's actually a useful philosophical distinction that can resolve it. The philosopher Bertrand Russell drew a line between two types of knowledge:</p>
<ul>
<li><em>Knowledge-that</em>: This is factual, descriptive knowledge that can be stated in propositions or truth claims. Think "book smarts."</li>
<li><em>Knowledge-by</em>: This is experiential, first-hand knowledge gained through direct acquaintance or immersion. The "street smarts" version.</li>
</ul>
<p>But there's a third type of knowledge that doesn't quite fit into Russell's framing:</p>
<ul>
<li><em>Knowledge-of</em>: Second-hand knowledge gained indirectly through conveyances like text and conversation rather than genuine experience.</li>
</ul>
<p>Language models like Claude have plenty of knowledge-of. Their training process exposes them to a vast trove of information extracted from the human-written text used as training . So when Claude tells you about the Napoleonic Wars or answers a coding question, it's drawing upon this second-hand knowledge-of.</p>
<p>Crucially, though, the AI doesn't have the grounded, experiential knowledge-by that a human historian or programmer would have. It also lacks the conceptual, justified true belief that comprises genuine knowledge-that. This lack of knowledge-that, combined with the requirement to generate syntactically correct language, is what leads the model to hallucinate.</p>
<p>Language serves to encode and transmit knowledge-that and knowledge-by between genuinely knowledgeable agents—people like you and me. AI, on the other hand, operates purely on the knowledge-of level--extracting apparent knowledge from linguistic patterns without the underlying semantics.</p>
<p>So while we can imprecisely say an AI“ knows" things in the sense of knowledge-of, it lacks true knowledge-that and certainly lacks knowledge-by. Its knowledge is second-hand, derived from linguistic representations of reality and not genuine understanding.</p>
<p>That's why I can honestly claim both "the AI knows" and "it doesn't actually know" without contradicting myself. What it "knows" is mere knowledge-of, not the real thing. Keeping this distinction in mind helps clarify what capabilities language models do and don't actually possess.</p>
8:["$","$La",null,{"header":"Beyond Knowledge-That: LLMs' Indirect Understanding - 2024-08-17","footer":["$","a",null,{"href":"../../","children":"Back"}],"className":"blog-page_BlogPage__t8vJA","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$b"}}]}]
7:null
