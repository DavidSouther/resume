<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/8089f6746c126975.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/c34f345ee7fcb4f3.css" data-precedence="next"/><link rel="preload" href="/_next/static/chunks/webpack-171fb46af9c86f7c.js" as="script" fetchPriority="low"/><script src="/_next/static/chunks/fd9d1056-edf0048f965d11ef.js" async=""></script><script src="/_next/static/chunks/596-e3269281f6a80cbc.js" async=""></script><script src="/_next/static/chunks/main-app-d3e28384a89d236c.js" async=""></script><link rel="preload" as="script" href="https://www.googletagmanager.com/gtag/js?id=G-6X1Z1L95D8"/><title>David Souther</title><meta name="description" content="davidsouther.com - resume, blog, playground"/><link rel="author" href="davidsouther.com"/><meta name="author" content="David Souther"/><link rel="manifest" href="/manifest.json"/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"/><link rel="stylesheet" href="/jiffies-css-bundle.min.css"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="container"><article class="blog-page_blogPage__HuM0o"><header><h3>Beyond Knowledge-That: LLMs&#x27; Indirect Understanding - 2024-08-17</h3></header><main><div><p>Why I Can Say "The AI Knows" but Still Claim "It Doesn't Really Know"?</p>
<p>We often talk about AI language models like they have real knowledge: "ChatGPT said the capital of France is Paris" or "Claude knows a lot about quantum physics." But then we also say things like "Language models don't actually understand what they're saying" or "They have no real knowledge, just patterns in data."</p>
<p>This seems like a contradiction, but there's actually a useful philosophical distinction that can resolve it. The philosopher Bertrand Russell drew a line between two types of knowledge:</p>
<ul>
<li><em>Knowledge-that</em>: This is factual, descriptive knowledge that can be stated in propositions or truth claims. Think "book smarts."</li>
<li><em>Knowledge-by</em>: This is experiential, first-hand knowledge gained through direct acquaintance or immersion. The "street smarts" version.</li>
</ul>
<p>But there's a third type of knowledge that doesn't quite fit into Russell's framing:</p>
<ul>
<li><em>Knowledge-of</em>: Second-hand knowledge gained indirectly through conveyances like text and conversation rather than genuine experience.</li>
</ul>
<p>Language models like Claude have plenty of knowledge-of. Their training process exposes them to a vast trove of information extracted from the human-written text used as training . So when Claude tells you about the Napoleonic Wars or answers a coding question, it's drawing upon this second-hand knowledge-of.</p>
<p>Crucially, though, the AI doesn't have the grounded, experiential knowledge-by that a human historian or programmer would have. It also lacks the conceptual, justified true belief that comprises genuine knowledge-that. This lack of knowledge-that, combined with the requirement to generate syntactically correct language, is what leads the model to hallucinate.</p>
<p>Language serves to encode and transmit knowledge-that and knowledge-by between genuinely knowledgeable agents—people like you and me. AI, on the other hand, operates purely on the knowledge-of level--extracting apparent knowledge from linguistic patterns without the underlying semantics.</p>
<p>So while we can imprecisely say an AI“ knows" things in the sense of knowledge-of, it lacks true knowledge-that and certainly lacks knowledge-by. Its knowledge is second-hand, derived from linguistic representations of reality and not genuine understanding.</p>
<p>That's why I can honestly claim both "the AI knows" and "it doesn't actually know" without contradicting myself. What it "knows" is mere knowledge-of, not the real thing. Keeping this distinction in mind helps clarify what capabilities language models do and don't actually possess.</p>
</div></main></article><script src="/_next/static/chunks/webpack-171fb46af9c86f7c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/8089f6746c126975.css\",{\"as\":\"style\"}]\n0:\"$L2\"\n"])</script><script>self.__next_f.push([1,"3:HL[\"/_next/static/css/c34f345ee7fcb4f3.css\",{\"as\":\"style\"}]\n"])</script><script>self.__next_f.push([1,"4:I{\"id\":7948,\"chunks\":[\"272:static/chunks/webpack-171fb46af9c86f7c.js\",\"971:static/chunks/fd9d1056-edf0048f965d11ef.js\",\"596:static/chunks/596-e3269281f6a80cbc.js\"],\"name\":\"default\",\"async\":false}\n6:I{\"id\":6628,\"chunks\":[\"272:static/chunks/webpack-171fb46af9c86f7c.js\",\"971:static/chunks/fd9d1056-edf0048f965d11ef.js\",\"596:static/chunks/596-e3269281f6a80cbc.js\"],\"name\":\"\",\"async\":false}\n7:I{\"id\":7767,\"chunks\":[\"272:static/chunks/webpack-171fb46af9c86f7c.js\",\"971:static/chunks/fd9d1056-edf0048f965d11ef.js\",\"5"])</script><script>self.__next_f.push([1,"96:static/chunks/596-e3269281f6a80cbc.js\"],\"name\":\"default\",\"async\":false}\n8:I{\"id\":7920,\"chunks\":[\"272:static/chunks/webpack-171fb46af9c86f7c.js\",\"971:static/chunks/fd9d1056-edf0048f965d11ef.js\",\"596:static/chunks/596-e3269281f6a80cbc.js\"],\"name\":\"default\",\"async\":false}\nb:I{\"id\":6852,\"chunks\":[\"13:static/chunks/13-a14f4f8488530559.js\",\"599:static/chunks/599-57c8542fa14d7dc7.js\",\"185:static/chunks/app/layout-3e0733cba7c240b0.js\"],\"name\":\"\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"2:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/8089f6746c126975.css\",\"precedence\":\"next\"}]],[\"$\",\"$L4\",null,{\"buildId\":\"20vnP8HKnii83kt-PpwqC\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/blog/knowledge_that_by_of\",\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"id\",\"knowledge_that_by_of\",\"d\"],{\"children\":[\"__PAGE__?{\\\"id\\\":\\\"knowledge_that_by_of\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[false,\"$L5\"],\"globalErrorComponent\":\"$6\",\"children\":[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[[\"$\",\"head\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"/jiffies-css-bundle.min.css\"}]}],[\"$\",\"body\",null,{\"className\":\"container\",\"children\":[[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"childProp\":{\"current\":[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",[\"id\",\"knowledge_that_by_of\",\"d\"],\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$L9\",\"$La\",null],\"segment\":\"__PAGE__?{\\\"id\\\":\\\"knowledge_that_by_of\\\"}\"},\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/c34f345ee7fcb4f3.css\",\"precedence\":\"next\"}]]}],\"segment\":[\"id\",\"knowledge_that_by_of\",\"d\"]},\"styles\":[]}],\"segment\":\"blog\"},\"styles\":[]}],[\"$\",\"$Lb\",null,{}]]}]]}],null]}]]\n"])</script><script>self.__next_f.push([1,"c:I{\"id\":8504,\"chunks\":[\"548:static/chunks/app/blog/[id]/page-5873cc6a307ffcb4.js\"],\"name\":\"\",\"async\":false}\nd:Ta8e,"])</script><script>self.__next_f.push([1,"\u003cp\u003eWhy I Can Say \"The AI Knows\" but Still Claim \"It Doesn't Really Know\"?\u003c/p\u003e\n\u003cp\u003eWe often talk about AI language models like they have real knowledge: \"ChatGPT said the capital of France is Paris\" or \"Claude knows a lot about quantum physics.\" But then we also say things like \"Language models don't actually understand what they're saying\" or \"They have no real knowledge, just patterns in data.\"\u003c/p\u003e\n\u003cp\u003eThis seems like a contradiction, but there's actually a useful philosophical distinction that can resolve it. The philosopher Bertrand Russell drew a line between two types of knowledge:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eKnowledge-that\u003c/em\u003e: This is factual, descriptive knowledge that can be stated in propositions or truth claims. Think \"book smarts.\"\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eKnowledge-by\u003c/em\u003e: This is experiential, first-hand knowledge gained through direct acquaintance or immersion. The \"street smarts\" version.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBut there's a third type of knowledge that doesn't quite fit into Russell's framing:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eKnowledge-of\u003c/em\u003e: Second-hand knowledge gained indirectly through conveyances like text and conversation rather than genuine experience.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLanguage models like Claude have plenty of knowledge-of. Their training process exposes them to a vast trove of information extracted from the human-written text used as training . So when Claude tells you about the Napoleonic Wars or answers a coding question, it's drawing upon this second-hand knowledge-of.\u003c/p\u003e\n\u003cp\u003eCrucially, though, the AI doesn't have the grounded, experiential knowledge-by that a human historian or programmer would have. It also lacks the conceptual, justified true belief that comprises genuine knowledge-that. This lack of knowledge-that, combined with the requirement to generate syntactically correct language, is what leads the model to hallucinate.\u003c/p\u003e\n\u003cp\u003eLanguage serves to encode and transmit knowledge-that and knowledge-by between genuinely knowledgeable agents—people like you and me. AI, on the other hand, operates purely on the knowledge-of level--extracting apparent knowledge from linguistic patterns without the underlying semantics.\u003c/p\u003e\n\u003cp\u003eSo while we can imprecisely say an AI“ knows\" things in the sense of knowledge-of, it lacks true knowledge-that and certainly lacks knowledge-by. Its knowledge is second-hand, derived from linguistic representations of reality and not genuine understanding.\u003c/p\u003e\n\u003cp\u003eThat's why I can honestly claim both \"the AI knows\" and \"it doesn't actually know\" without contradicting myself. What it \"knows\" is mere knowledge-of, not the real thing. Keeping this distinction in mind helps clarify what capabilities language models do and don't actually possess.\u003c/p\u003e\n"])</script><script>self.__next_f.push([1,"a:[\"$\",\"$Lc\",null,{\"post\":{\"id\":\"knowledge_that_by_of\",\"date\":\"2024-08-17T00:00:00.000Z\",\"title\":\"Beyond Knowledge-That: LLMs' Indirect Understanding\",\"body\":\"$d\",\"show\":true}}]\n"])</script><script>self.__next_f.push([1,"5:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"David Souther\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"davidsouther.com - resume, blog, playground\"}],[\"$\",\"link\",\"3\",{\"rel\":\"author\",\"href\":\"davidsouther.com\"}],[\"$\",\"meta\",\"4\",{\"name\":\"author\",\"content\":\"David Souther\"}],[\"$\",\"link\",\"5\",{\"rel\":\"manifest\",\"href\":\"/manifest.json\"}],[\"$\",\"meta\",\"6\",{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: light)\",\"content\":\"white\"}],[\"$\",\"meta\",\"7\",{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: dark)\",\"content\":\"black\"}],[\"$\",\"meta\",\"8\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"9:null\n"])</script></body></html>