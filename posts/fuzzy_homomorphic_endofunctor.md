---
title: Fuzzy Homomorphic Endofunctors
date: 2024-04-04
summary: LLMs are exploratory fuzzy homomorphic endofunctors mapping between points on the manifold of syntactically valid documents, traversing diverse paths of meaning.
---

Git commits are [isomorphic contours in source-code phase space](https://www.reddit.com/r/programming/comments/embdf/git_complicated_of_course_not_commits_map_to/), and so do LLMs!

Consider all the possible sequences of characters that make up syntactically valid text in a given natural language. This set forms a high-dimensional space. However, not all points in this vast space correspond to coherent, meaningful content a reader would understand. The set of all well-formed documents defines a complex, convoluted manifold - a lower-dimensional continuous surface embedded within that higher space.

[![Claude: A Fuzzy Homomorphic Endofunctor](/fuzzy_llm.png)](/fuzzy_llm.webp)

_Claude as a fuzzy homomorphic endofunctor._

Large language models (LLMs) like ChatGPT or Claude can be viewed as intelligent agents that explore and navigate this natural language manifold. When given an initial prompt, an LLM is effectively placed at a specific point on this manifold's surface corresponding to coherent text so far. Its neural architecture allows it to model local statistical patterns and transition to nearby points, generating contextually relevant text while remaining on the manifold - adhering to syntactic and semantic constraints.

However, from any given point, the LLM is not locked into a single predetermined path. There exists a fuzzy radius within which it can branch off onto distinctly different but equally valid trajectories over the manifold's surface. By leveraging vast neural weights trained on immense language data, LLMs gain the ability to fluidly navigate and explore multiple plausible communicative paths through the high-dimensional language surface.

Altering the initial prompt is akin to repositioning the LLM on another part of the manifold to reset its constrained exploration. This flexibility to search out diverse meaningful continuations from any context, while respecting linguistic constraints, underlies the generalization and open-ended creative expression abilities of large language models.

In essence, LLMs are semi-intelligent multidimensional explorers intelligently mapping out the convoluted hypersurface representing a natural language's full scope of meaningful communication within a vast formal space of possible texts. Their generative behavior stems from learned high-dimensional geometry inextricably tied to human language itself.
