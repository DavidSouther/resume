---
title: "Beyond Knowledge-That: LLMs' Indirect Understanding"
date: 2024-08-17
summary: We say LLM AIs "know" things, yet also claim they "don't actually know." This isn't contradictory—it's about different knowledge types. LLMs have knowledge-of from data, but lack humans' genuine knowledge-that (justified true belief) and knowledge-by (direct experience). Key distinction!
---

Why I Can Say "The AI Knows" but Still Claim "It Doesn't Really Know"?

We often talk about AI language models like they have real knowledge: "ChatGPT said the capital of France is Paris" or "Claude knows a lot about quantum physics." But then we also say things like "Language models don't actually understand what they're saying" or "They have no real knowledge, just patterns in data."

This seems like a contradiction, but there's actually a useful philosophical distinction that can resolve it. The philosopher Bertrand Russell drew a line between two types of knowledge:

- _Knowledge-that_: This is factual, descriptive knowledge that can be stated in propositions or truth claims. Think "book smarts."
- _Knowledge-by_: This is experiential, first-hand knowledge gained through direct acquaintance or immersion. The "street smarts" version.

But there's a third type of knowledge that doesn't quite fit into Russell's framing:

- _Knowledge-of_: Second-hand knowledge gained indirectly through conveyances like text and conversation rather than genuine experience.

Language models like Claude have plenty of knowledge-of. Their training process exposes them to a vast trove of information extracted from the human-written text used as training . So when Claude tells you about the Napoleonic Wars or answers a coding question, it's drawing upon this second-hand knowledge-of.

Crucially, though, the AI doesn't have the grounded, experiential knowledge-by that a human historian or programmer would have. It also lacks the conceptual, justified true belief that comprises genuine knowledge-that. This lack of knowledge-that, combined with the requirement to generate syntactically correct language, is what leads the model to hallucinate.

Language serves to encode and transmit knowledge-that and knowledge-by between genuinely knowledgeable agents—people like you and me. AI, on the other hand, operates purely on the knowledge-of level--extracting apparent knowledge from linguistic patterns without the underlying semantics.

So while we can imprecisely say an AI“ knows" things in the sense of knowledge-of, it lacks true knowledge-that and certainly lacks knowledge-by. Its knowledge is second-hand, derived from linguistic representations of reality and not genuine understanding.

That's why I can honestly claim both "the AI knows" and "it doesn't actually know" without contradicting myself. What it "knows" is mere knowledge-of, not the real thing. Keeping this distinction in mind helps clarify what capabilities language models do and don't actually possess.
